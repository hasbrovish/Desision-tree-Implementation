{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desicion Tree Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " #creating dataset\n",
    "#df = [\n",
    "#     ['True', 'False', 'True'],\n",
    "#     ['False', 'True', 'True'],\n",
    "#     ['False', 'False', 'False'],\n",
    "#     ['True', 'True', 'True'],\n",
    "    \n",
    "#]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating dataset\n",
    "df = [\n",
    "     ['Green', 3, 'Apple'],\n",
    "     ['Yellow', 3, 'Apple'],\n",
    "     ['Red', 1, 'Grape'],\n",
    "     ['Red', 1, 'Grape'],\n",
    "     ['Yellow', 3, 'Lemon'],\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Green', 3, 'Apple'],\n",
       " ['Yellow', 3, 'Apple'],\n",
       " ['Red', 1, 'Grape'],\n",
       " ['Red', 1, 'Grape'],\n",
       " ['Yellow', 3, 'Lemon']]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# changing training dataset to pandas datafarme\n",
    "df=pd.DataFrame(df,columns = [\"X1\", \"X2\", \"Result\"])\n",
    "\n",
    "# features list store, the features present in dataset\n",
    "features=[\"X1\", \"X2\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# X is features trainig set\n",
    "X=df.iloc[:,:-1]\n",
    "\n",
    "# Y is training column\n",
    "Y=df.iloc[:,-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# target_entopy function calulates the entropy \n",
    "def target_entropy(class_atr):\n",
    "    \n",
    "    # number of differnt classes present is get using set function, \n",
    "    # here a store the list of differnt class present\n",
    "    a=set(class_atr)\n",
    "    \n",
    "    total=len(class_atr)\n",
    "    \n",
    "    # entropy variable is used to store net entropy of col\n",
    "    entropy=0\n",
    "    \n",
    "    for i in a:\n",
    "        t=0\n",
    "        # nested loops are used to calculate net count of particular class in column\n",
    "        for j in range(total):\n",
    "            if class_atr[j]==i:\n",
    "                t=t+1\n",
    "                \n",
    "        # t stores no of particular class type present in column\n",
    "        entropy=entropy-(t*math.log2(t/total))/total\n",
    "        \n",
    "    # net entropy is returned to calling function\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# info function return information required after splitting on feature\n",
    "def info_after_split(x,y,feature):\n",
    "    \n",
    "    #info_req_split is used to store net inforamition after split\n",
    "    info_req_split=0\n",
    "    total=len(y)  # total stores size of col\n",
    "    \n",
    "    a=set(x[feature])  # a store the all differnt classes present in column\n",
    "    \n",
    "    l=x[feature].values  #  feature column values are stored in 1d list\n",
    "    \n",
    "    y_1d=y.values   # target column values are store in 1d list\n",
    "    \n",
    "    for i in a:\n",
    "        t=0    # t counts total no of instances having class i feature\n",
    "        s=[]     # empty list is created which stores all target values asscociated with feature class i\n",
    "        \n",
    "        for j in range(len(l)):\n",
    "            if l[j]==i:\n",
    "                s.append(y_1d[j])    # on match target class accocatoited with feature class is appended in the list\n",
    "                t=t+1\n",
    "        info_req_split =info_req_split +(t*(target_entropy(s)))/total  # weight of net info gain by class i is added\n",
    "    \n",
    "    #net info gain is returned\n",
    "    return info_req_split \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# split info is calcuated for particular feature\n",
    "def split_info(x,y,feature):\n",
    "    \n",
    "    # net_split_info stores the net split info on feature \n",
    "    net_split_info=0\n",
    "    total=len(y)\n",
    "    \n",
    "    a=set(x[feature])  # set stores no of differnt classes prsent in feature col of traing set\n",
    "    l=x[feature].values   # feature columns values are stored in 1d array\n",
    "    y_1d=y.values     # target column values are stored in 1d array\n",
    "    \n",
    "    for i in a:     # iteration is performed on each class to count no of occurance\n",
    "        t=0 \n",
    "        for j in range(len(l)):  \n",
    "            if l[j]==i:\n",
    "                t=t+1   # t is incremented if there is match( same classes )\n",
    "        net_split_info=net_split_info-((t)*(math.log2(t/total)))/total  # split info contributed by class i is added to net split info\n",
    " \n",
    "    return net_split_info\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gain function calculate gain ratio for feature\n",
    "#  X is feature dataframe\n",
    "# Y is target, feature is feature for which gain ratio is to be calculated\n",
    "\n",
    "def gain(X,Y,feature):\n",
    "    \n",
    "    y_id=Y.values  # y_id stores element present in target column in 1d list\n",
    "    info_req=(target_entropy(y_id))  # info_req is info gain if there is no splitting\n",
    "    \n",
    "    a=set(X[feature])  # a is set which stores differnt class present in feature  column\n",
    "    l=X[feature].values   # l store elemnt present in feature col in 1d list\n",
    "    \n",
    "    info_f = info_after_split(X,Y,feature)  # info_f stores info gain after split\n",
    "    \n",
    "    info_gain = info_req-info_f        # info_gain can be calculated using formula =info_required-(info_required after split)\n",
    "    \n",
    "    split_info_value=split_info(X,Y,feature)  # split info for feature f is calulated\n",
    "    \n",
    "    if split_info_value!=0:\n",
    "        gain_ratio=info_gain/split_info_value    # gain ratio is calculated using formula\n",
    "    else:\n",
    "        gain_ratio=0\n",
    "    \n",
    "    return info_req,gain_ratio   # current entropy and gain_ ratio for feature is returned\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# count_print functio is used here to print no of differnt classes present in target\n",
    "def count_print(Y):\n",
    "    a=set(Y)\n",
    "    y=Y.values\n",
    "    for i in a:\n",
    "        t=0\n",
    "        for j in y:\n",
    "            if j==i:\n",
    "                t=t+1\n",
    "        print(\"Count of\",i,\"=\",t)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# decision tree rescursive function is used here to operate on differnt level recursively\n",
    "# X is training dataframe\n",
    "# Y is target, feature is list of feature, level variable is used to print node level\n",
    "def decisionTree(X,Y,feature,level):\n",
    "    \n",
    "    # base case:  if feature list has no element \n",
    "    if (len(feature))==0:\n",
    "        print(\"Current Entropy is = 0.0\")\n",
    "        print(\"Reached Leaf Node\")\n",
    "        print()\n",
    "        return\n",
    "    \n",
    "    # base case: if target is pure , means single class is present in target col\n",
    "    if len(set(Y))==1:\n",
    "        print(\"Current Entropy is = 0.0\")\n",
    "        print(\"Reached Leaf Node\")\n",
    "        print()\n",
    "        return\n",
    "    \n",
    "    ma=-1000000000   # ma variable is used to exract out maximum gain ratio for particular feature\n",
    "    y=Y.values      \n",
    "    t=feature[0]   # t variable is used to exract out feature for which gain ratio is maximun \n",
    "    \n",
    "    # iteratively checking for which feature gain ratio is maximum\n",
    "    for i in feature:\n",
    "        entropy,k=gain(X,Y,i)  # gain() is used to calculate gain, entropy stores current entropy of target\n",
    "        if k>ma:\n",
    "            t=i\n",
    "            ma=k\n",
    "\n",
    "    # after loop ends t store feature for which there is maximum gain_ratio\n",
    "    # after loop ma stores maximum gain ratio\n",
    "    \n",
    "    a=set(X[t])        # a is used to store no of different classes present in feature \"t column\n",
    "    feature.remove(t)    # feature t is removed for feature list,\n",
    "    level=level+1       # as now splitting will be called on feature \"t\", so level is incremented\n",
    "\n",
    "    print(\"Current Entropy is =\",entropy)  \n",
    "    print(\"Splitting on feature\",t,\"with gain ratio\",ma)\n",
    "    print()\n",
    "    \n",
    "    Z=X.join(Y)           # training dataframe and target is joined so that is differnt split dataframe can be filterd out easily\n",
    "    \n",
    "    for i in a:\n",
    "        dl=Z[Z[t]==i]       # same class row are filtered out and stored in dl(dataframe)\n",
    "        # dl is again devided in feature and target\n",
    "        y=dl.iloc[:,-1]      \n",
    "        x=dl.iloc[:,:-1]\n",
    "        \n",
    "        del x[t]   # feature \"t\" column is removed from split feature dataframe(x)\n",
    "        print(\"Level\",level)\n",
    "        count_print(y)\n",
    "        decisionTree(x,y,feature,level)  # recursion is called on spitted dataframe, with incremented level\n",
    "        \n",
    "# function end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 0\n",
      "Count of Apple = 2\n",
      "Count of Lemon = 1\n",
      "Count of Grape = 2\n",
      "Current Entropy is = 1.5219280948873621\n",
      "Splitting on feature X2 with gain ratio 0.9999999999999998\n",
      "\n",
      "Level 1\n",
      "Count of Grape = 2\n",
      "Current Entropy is = 0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "Level 1\n",
      "Count of Apple = 2\n",
      "Count of Lemon = 1\n",
      "Current Entropy is = 0.9182958340544896\n",
      "Splitting on feature X1 with gain ratio 0.274017542121281\n",
      "\n",
      "Level 2\n",
      "Count of Apple = 1\n",
      "Count of Lemon = 1\n",
      "Current Entropy is = 0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "Level 2\n",
      "Count of Apple = 1\n",
      "Current Entropy is = 0.0\n",
      "Reached Leaf Node\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Level 0\")\n",
    "count_print(Y)\n",
    "level=0\n",
    "decisionTree(X,Y,features,level)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
